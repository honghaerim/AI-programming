{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1+B8J0ypTfVxSw2uTS/pe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/honghaerim/AI-programming/blob/main/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D_10%EC%A3%BC%EC%B0%A8%EA%B3%BC%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# 훈련 데이터\n",
        "x_train = np.array([[0], [1], [2]])\n",
        "y_train = 3 * x_train + 1\n",
        "\n",
        "# 테스트 데이터\n",
        "x_test = np.array([[3], [4], [5]])\n",
        "y_test = 3 * x_test + 1\n",
        "\n",
        "# 최적화 도구와 손실 함수 정의\n",
        "optimizers = ['sgd', 'adam', 'adagrad', 'rmsprop']\n",
        "loss_functions = [\n",
        "    'mean_squared_error',\n",
        "    'binary_crossentropy',\n",
        "    'categorical_crossentropy',\n",
        "    'mean_absolute_error',\n",
        "    keras.losses.Huber()  # Huber Loss를 tf.keras.losses.Huber()로 사용\n",
        "]\n",
        "\n",
        "# 결과 저장을 위한 딕셔너리\n",
        "results = {}\n",
        "\n",
        "# 최적화 도구와 손실 함수를 통해 반복\n",
        "for optimizer in optimizers:\n",
        "    for loss in loss_functions:\n",
        "        try:\n",
        "            # 모델 생성\n",
        "            model = keras.models.Sequential()\n",
        "            model.add(keras.layers.Dense(4, input_shape=(1,)))\n",
        "            model.add(keras.layers.Dense(1))\n",
        "\n",
        "            # 모델 컴파일\n",
        "            model.compile(\n",
        "                loss=loss,\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mae']  # Mean Absolute Error를 추가하여 평가 지표로 사용\n",
        "            )\n",
        "\n",
        "            # 모델 훈련\n",
        "            model.fit(x_train, y_train, epochs=30, batch_size=2, verbose=0)\n",
        "\n",
        "            # 예측\n",
        "            pred = model.predict(x_test)\n",
        "\n",
        "            # 결과 저장\n",
        "            results[(optimizer, loss)] = pred.flatten()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"오류 발생 - 최적화 도구: {optimizer}, 손실 함수: {loss} - {e}\")\n",
        "\n",
        "# 결과 출력\n",
        "for key, value in results.items():\n",
        "    print(f'최적화 도구: {key[0]}, 손실 함수: {key[1]}, 예측: {value}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF2zKJg90sjT",
        "outputId": "21bc318d-1954-4715-8ad2-b5c4c67a8921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "최적화 도구: sgd, 손실 함수: mean_squared_error, 예측: [ 9.967763 12.947817 15.927868]\n",
            "최적화 도구: sgd, 손실 함수: binary_crossentropy, 예측: [3.6505601 4.527177  5.4037933]\n",
            "최적화 도구: sgd, 손실 함수: categorical_crossentropy, 예측: [nan nan nan]\n",
            "최적화 도구: sgd, 손실 함수: mean_absolute_error, 예측: [0.8065169  0.6872355  0.56795406]\n",
            "최적화 도구: sgd, 손실 함수: <keras.src.losses.losses.Huber object at 0x7fed172cead0>, 예측: [ 8.034693 10.213352 12.392012]\n",
            "최적화 도구: adam, 손실 함수: mean_squared_error, 예측: [1.3262401 1.7407124 2.155185 ]\n",
            "최적화 도구: adam, 손실 함수: binary_crossentropy, 예측: [-1.9723885 -2.6298518 -3.2873144]\n",
            "최적화 도구: adam, 손실 함수: categorical_crossentropy, 예측: [nan nan nan]\n",
            "최적화 도구: adam, 손실 함수: mean_absolute_error, 예측: [2.6164606 3.397837  4.179213 ]\n",
            "최적화 도구: adam, 손실 함수: <keras.src.losses.losses.Huber object at 0x7fed172cead0>, 예측: [-0.9605676 -1.3559374 -1.7513074]\n",
            "최적화 도구: adagrad, 손실 함수: mean_squared_error, 예측: [-3.476881  -4.6479354 -5.8189898]\n",
            "최적화 도구: adagrad, 손실 함수: binary_crossentropy, 예측: [ -6.938347  -9.251129 -11.563911]\n",
            "최적화 도구: adagrad, 손실 함수: categorical_crossentropy, 예측: [nan nan nan]\n",
            "최적화 도구: adagrad, 손실 함수: mean_absolute_error, 예측: [1.0907031 1.438843  1.786983 ]\n",
            "최적화 도구: adagrad, 손실 함수: <keras.src.losses.losses.Huber object at 0x7fed172cead0>, 예측: [2.7467856 3.646678  4.5465693]\n",
            "최적화 도구: rmsprop, 손실 함수: mean_squared_error, 예측: [2.250732  2.937374  3.6240168]\n",
            "최적화 도구: rmsprop, 손실 함수: binary_crossentropy, 예측: [-0.328673   -0.43823087 -0.5477886 ]\n",
            "최적화 도구: rmsprop, 손실 함수: categorical_crossentropy, 예측: [nan nan nan]\n",
            "최적화 도구: rmsprop, 손실 함수: mean_absolute_error, 예측: [3.7285125 4.882683  6.036854 ]\n",
            "최적화 도구: rmsprop, 손실 함수: <keras.src.losses.losses.Huber object at 0x7fed172cead0>, 예측: [1.6908305 2.1612117 2.6315925]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) 손실 함수의 의미\n",
        "손실 함수(Loss Function)는 모델의 예측 결과와 실제 정답(레이블) 간의 차이를 수치적으로 측정하는 함수입니다. 손실 함수의 주요 목적은 모델이 얼마나 잘 예측하고 있는지를 평가하는 것입니다. 손실 값이 낮을수록 모델의 예측이 실제값에 가까워지며, 손실 값이 높으면 모델의 예측이 부정확하다는 것을 의미합니다.\n",
        "\n",
        "2) 최적화 함수의 의미\n",
        "최적화 함수(Optimizer)는 모델의 가중치를 업데이트하여 손실 함수를 최소화하는 방법을 결정하는 알고리즘입니다. 즉, 모델이 학습하는 동안 손실 값을 줄이기 위해 가중치를 조정하는 역할을 합니다.\n",
        "\n",
        "3) 딥러닝에서 손실 함수와 최적화 함수의 역할\n",
        "손실 함수\n",
        "모델이 얼마나 잘 학습하고 있는지를 평가하는 기준이 됩니다.\n",
        "손실 함수를 통해 모델의 성능을 정량화할 수 있으며, 학습 과정에서 이 값을 최소화하는 방향으로 모델이 업데이트됩니다.\n",
        "최적화 함수\n",
        "손실 함수를 최소화하기 위한 가중치 업데이트 방법을 결정합니다.\n",
        "적절한 최적화 함수를 선택하는 것은 학습의 속도와 수렴성에 큰 영향을 미칩니다.\n",
        "최적화 함수는 가중치 업데이트의 방향과 크기를 조절하여 손실 함수의 최소값을 찾도록 도와줍니다."
      ],
      "metadata": {
        "id": "Wl5H-2QpthPj"
      }
    }
  ]
}